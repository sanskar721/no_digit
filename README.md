# no_digit

# My First Neural Network Project

Welcome to my first neural network project on GitHub!

## Project Overview
This project is a simple implementation of a digit recognizer using the MNIST dataset. The main focus is to demonstrate a basic neural network model built with perceptrons. The model employs two different activation functions: tanh and softmax.

### Activation Functions Used:
1. **Tanh Activation Function**: 
   - Tanh, short for hyperbolic tangent, squashes the input values between -1 and 1. It's commonly used in neural networks for its non-linear properties.

2. **Softmax Activation Function**:
   - Softmax is often used as the output layer activation function in multi-class classification problems. It normalizes the output into a probability distribution over predicted classes.

## About MNIST Dataset
The MNIST dataset is a collection of 28x28 grayscale images of handwritten digits ranging from 0 to 9. It's a popular dataset for beginners in machine learning and serves as a benchmark for testing various algorithms.

## How to Use
1. Clone this repository to your local machine.
2. Ensure you have the required dependencies installed.
3. Run the provided scripts or notebooks to train and evaluate the neural network model.
4. Experiment with different configurations, activation functions, or even try implementing additional features to improve the model's performance.

## Dependencies
- Python 3.x
- numpy
- matplotlib
- scikit-learn

## Acknowledgements
- The MNIST dataset used in this project is widely available and is often used in educational and research settings.
- Special thanks to the open-source community for providing valuable resources and tutorials on neural networks and machine learning.

Feel free to explore the code and provide any feedback or suggestions!
